\documentclass{../template/texnote}

\title{\textbf{\capitalisewords{AI over coffee}}}

\begin{document}
    \maketitle \currentdoc{note}
    %<*note>

	%\section{Introduction}
This article is a coffee table discussion between two friends, Annie and Marty. Annie is an inquisitive learner who just met her long-time friend Marty, who is doing his PhD. in artificial intelligence. Annie is amused by the sudden rise of Artificial Intelligence and wants to understand the underlying concepts or principles that drive its development. She also wants to be abreast of the developments in the field that have led to the current state of affairs. The conversation is divided into sections for the reader to follow along easily.

\section{The dumb and powerful computers of the past} 
\noindent

\textbf{Annie}: Hi Marty. Can you explain to me what this whole A.I. thing is? What is all the buzz about?

\textbf{Marty}: Ya sure. We are probably on the brink of another industrial revolution. The invention of the steam engine is considered to have brought about the first industrial revolution. The invention of electricity was another big step that radically changed the nature of machines. The invention of the transistor marked the third industrial revolution, or what we call the digital revolution. These transistors power the I.C.s or chips found in most electronics nowadays, including modern computers. The new revolution we are talking about has to do with the age of intelligent machines, and A.I., as you might know, stands for Artificial Intelligence.

\textbf{Annie}: Oh! Intelligent machines, so that's what it is about. But I was under the impression that computers were already pretty intelligent. I saw this movie recently, ``The Imitation Game", where they talk about the history of computers and how they were developed during the Second World War era to break the secret code used by the Germans. Is that true? 

\textbf{Marty}: Yes, it is very accurate. It is also true that the computers of yesteryears were indeed very capable. They have helped astronauts land on the moon, enabled metrologists to predict the weather, and cosmologists to simulate the universe's evolution. As late as 1997, a chess-playing computer called Deep Blue was developed, which beat the reigning world champion Gary Kasparov in his own game. But all of these were done using brute-force techniques and not by intelligent machines.

\section{New-Age computers that can learn}
\noindent

\textbf{Annie}: Oh yeah, I remember hearing about Deep Blue beating Kasparov in the news. So do you mean to say that it was not an intelligent computer that did that?

\textbf{Marty}: Yes.

\textbf{Annie}: So what are these intelligent machines capable of doing that the previous-generation computers couldn't?

\textbf{Marty}: Even the most sophisticated machines of the previous generation failed spectacularly at tasks that even a five-year-old kid could do. These are things like image recognition, speech recognition, and natural language processing. Today's machines are considered intelligent because they are able to do a lot of such tasks that were considered too hot to handle for the older generation of computers. 

\textbf{Annie}: Oh burn! So how do the new-age computers shine where previous-generation computers failed?

\textbf{Marty}: Traditional computers were powerful because of two significant things - brute strength and explicit algorithms.
The brute strength of the computer comes from its mind-boggling speed;  the modern computer is capable of executing about 5 billion instructions per second, and it's able to crunch large numbers for hours on end without tiring out or getting brain fatigued. An algorithm is something like that recipe you use for cooking a Shakshouka. A set of rules or instructions given to the computer to solve a specific problem. When it is written out in a language that the computer can understand, we call it computer code. With new-age computers, however, we don't need such algorithms. Instead, we have something called learning algorithms. 

\textbf{Annie}: So why are learning algorithms important for creating intelligent machines?

\textbf{Marty}: If we only had computers that relied on explicit algorithms, that would mean that computers could only solve problems that humans knew how to solve in the first place. When it comes to intelligence, you see that although human intelligence has a lot to do with evolution and our genes, one defining characteristic of intelligence is the ability to learn from experience. A child's brain is somewhat of a clean slate that has just the ability to learn from its environment, hard-wired into it. Everything else that makes it intelligent is learned during the course of its life, either from its own experiences or from the experiences of others. 

\textbf{Annie}: But even if they could achieve these tasks, wouldn't that make these computers only as bright as a five-year-old? 

\section{A.I. in science and engineering}
\noindent


\textbf{Marty}: You may be wrong about that. The ability to learn, paired with the brute strength of computers, results in a deadly combination. Learning to do even the most mundane tasks that people do every day can spark a revolution. The reason for all these fancy applications and the hype that A.I. has been generating recently has to do with people trying to build learning capabilities in their respective domains. Self-driving cars, language translators, spam detection, recommendation engines, etc., are all such examples. But the same techniques can be applied in scientific research, leading to even more exciting applications. 

\textbf{Annie}: Ohh. Using A.I. for research? That sounds exciting. What are the research problems that can be tackled using A.I.?

\textbf{Marty}: Glad you asked. There are lots of such applications. For instance, in medical research, they are used for cancer diagnosis. Using C.T. scans from both people identified with cancer and similar scans from healthy people, an intelligent machine can be trained to identify patterns in the data that evade even the best oncologists. A.I. is used for drug discovery to identify new drug candidates and predict how they will interact with biological systems. A.I. systems have been used to analyze large amounts of genetic data to identify genetic variants associated with specific diseases and predict an individual's risk of developing a disease. A key part of such research involves the 3D structure identification of proteins from their 2D images, and A.I. is used to do exactly that. In astrophysics, you might remember that the Nobel prize was given for the discovery of gravitational waves. 

\textbf{Annie}: Yeah I remember that. Those were theoretically predicted by Albert Einstein but not observed until recently, right?

\textbf{Marty}: Yes. And even there, A.I. was used to identify the true signals buried under a lot of noise. They are also used to analyze large amounts of telescope data to study the properties of stars, galaxies, and other celestial objects. 

\section{Primitive learning algorithms}
\noindent


\textbf{Annie}: Wow. All these sound exciting. So if these "learning algorithms"  are the workhorses behind all of these things that A.I. is capable of achieving, what are they actually, and how do they work?

\textbf{Marty}: Instead of having domain-specific rules hard-coded into the machines, the learning algorithms enable computers to make blunders, evaluate the cost of their errors by comparing them with true values, and keep on iterating until the accuracy becomes sufficiently high. In most of these cases, the computers try to arrive at rules by finding patterns in the data shown to them. Remember those questions that often come up in competitive exams where a number is omitted from a sequence of numbers, and we are asked to predict the missing one?

\textbf{Annie}: Yup. Those are really hard at times. 

\textbf{Marty}: Ya. But those are a piece of cake if you use learning algorithms to solve them. The curve fitting you learn in mathematics is an example of such a learning algorithm. 

\textbf{Annie}: Isn't curve fitting what we do when you take observations on graph paper during a science experiment and later try to fit a curve that accommodates all these data points?

\textbf{Marty}: Exactly. 

\textbf{Annie}: So are these the kind of algorithms that A.I. nowadays uses?

\textbf{Marty}: Actually, curve-fitting is one of the simplest and most primitive learning algorithms. Many years of research in this field have led to better learning algorithms. 

\textbf{Annie}: But what do you mean by a better algorithm?

\textbf{Marty}: Curve-fitting is mostly used in cases where the form of the relationship between the variables is known a priori. Most often than not, it is used in cases where the relationship is linear. Additionally, these methods are sensitive to the presence of outliers or noise in the data. 

\section{Artificial Neural Networks}
\noindent

\textbf{Annie}: So what better algorithms have people come up with until now?

\textbf{Marty}: The state-of-the-art in artificial intelligence is what we call artificial neural networks or ANN. It is a kind of bio-mimicry where the algorithm is derived from the working of the human brain, which is basically a large network of neurons. Although our knowledge about the brain is quite limited, the general idea we have is that the learning process in the brain is related to what we call neuroplasticity. 

\textbf{Annie}: And what do you mean by neuroplasticity in our brains?

\textbf{Marty}: Our brain is a large collection of neurons. The term "large" would be an oversimplification; typically, there are about 100 billion neurons with about 100 trillion synapses, which are the connections between the neurons.

\textbf{Annie}: Wow! That's such a mind-boggling number.

\textbf{Marty}: Yes it is. Each time a child sees an image, say that of a dog, some of these neurons fire together. The neurons that fire together become well-connected. The connection strength between certain other neurons might decrease. In this way, the idea of a dog is stored in our brain in the strength of the synapses between individual neurons. 

\textbf{Annie}: Oh. So that is probably the reason why different areas of the brain store different memories and have different functions. 

\textbf{Marty}: Yes. And as more and more data about dogs comes in through images, the brain can learn a very strong pattern that will enable it to identify all kinds of dogs in the future, even those it hasn't seen yet. 

\textbf{Annie}: So why does my one-year-old nephew mistake the moon for a ball?

\textbf{Marty}: Yes. That can happen in the initial phases of development when the brain is slowly learning to identify certain features that can help it to distinguish between objects. As more and more data flows into the brain, it can identify objects at a distance and quickly conclude that it's more probable to be something in the sky rather than a ball. 

\textbf{Annie}: Oh, this conversation has been very interesting. When can we talk more about this?

\textbf{Marty}: Soon enough I guess. 

\textbf{Annie}: Then it's a bye for now. Bye, Marty. 

\textbf{Marty}: Bye Annie. 
    %</note>
    \printbibliography
\end{document}
